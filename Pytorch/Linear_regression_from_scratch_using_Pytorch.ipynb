{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "d8V9GyMtRir1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making training data\n",
        "#input variables ---> (temp,rainfall,humidity)---> yield of apple and orange crop\n",
        "\n",
        "inputs = np.array([\n",
        "    [73,67,43],\n",
        "    [91,88,64],\n",
        "    [87,134,58],\n",
        "    [102,43,37],\n",
        "    [69,96,70]\n",
        "],dtype='float32')\n"
      ],
      "metadata": {
        "id": "-0MFfjEmRsu4"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = np.array([\n",
        "    [56,70],\n",
        "    [81,101],\n",
        "    [119,113],\n",
        "    [22,37],\n",
        "    [103,119]\n",
        "],dtype=np.float32)"
      ],
      "metadata": {
        "id": "8Xmle3ETTVQC"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "target = torch.from_numpy(target)"
      ],
      "metadata": {
        "id": "baIjRWxhTz87"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(2,3,requires_grad=True)\n",
        "b = torch.randn(2,requires_grad=True)"
      ],
      "metadata": {
        "id": "kVZYHw5IUQEY"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDIttNCUjJO",
        "outputId": "9d3e0ed4-5c28-4c1e-d586-a4aa22760b98"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.8135,  1.2783,  0.3731],\n",
            "        [ 0.3546, -0.1219,  1.8978]], requires_grad=True) tensor([0.0118, 0.9895], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "  return x @ w.t() + b"
      ],
      "metadata": {
        "id": "mTghKJjJUlf6"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j_-FeJMVJ0d",
        "outputId": "017d5f7b-80c9-4302-9fb2-ac1c48025161"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -30.6879,  100.3114],\n",
            "        [ -28.6521,  143.9871],\n",
            "        [  35.1640,  125.5732],\n",
            "        [-116.1983,  102.1353],\n",
            "        [  23.7110,  146.5965]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function mse\n",
        "def mse(actual,target):\n",
        "  diff = actual - target\n",
        "\n",
        "  return torch.sum(diff*diff)/diff.numel()\n"
      ],
      "metadata": {
        "id": "VvXZPHdIVPsN"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Error = loss = MSE\n",
        "loss = mse(preds,target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtsLM9RpXgL4",
        "outputId": "af190462-f04b-4a54-b3f5-177731cfcbb7"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5988.1289, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "oUY0PP8KXjnk"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0HF0mwzbznK",
        "outputId": "cd988fb9-2b08-42b7-dc19-eb58dc792e0b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.8135,  1.2783,  0.3731],\n",
            "        [ 0.3546, -0.1219,  1.8978]], requires_grad=True)\n",
            "tensor([[-8633.4902, -8049.1519, -5254.2729],\n",
            "        [ 3153.2795,  2589.7261,  1825.1160]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J85YbvVDb8m2",
        "outputId": "ad746f92-2f46-438d-dd7b-03035792d37a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0118, 0.9895], requires_grad=True)\n",
            "tensor([-99.5326,  35.7207])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_araCnOIcQTp",
        "outputId": "3bdf89d6-b6d3-4648-e9c0-68d56e13b08c"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adjust parameters\n",
        "preds = model(inputs)\n",
        "print(preds)\n",
        "loss = mse(target,preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdzxRA-ihzvt",
        "outputId": "a1c58114-cc9c-47ed-ead1-432fbaced3b5"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -30.6879,  100.3114],\n",
            "        [ -28.6521,  143.9871],\n",
            "        [  35.1640,  125.5732],\n",
            "        [-116.1983,  102.1353],\n",
            "        [  23.7110,  146.5965]], grad_fn=<AddBackward0>)\n",
            "tensor(5988.1289, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrW7ZO--h_v8",
        "outputId": "859019a4-5836-4605-d7df-46b053cc3820"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-8633.4902, -8049.1519, -5254.2729],\n",
            "        [ 3153.2795,  2589.7261,  1825.1160]])\n",
            "tensor([-99.5326,  35.7207])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5\n",
        "with torch.no_grad():\n",
        "  w-=w.grad * learning_rate\n",
        "  b-=b.grad * learning_rate\n",
        "\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ],
      "metadata": {
        "id": "WYWbBu5NkF_W"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "loss = mse(target,preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-dwZE6dl0gf",
        "outputId": "0125a007-7b77-425c-9abb-1fecc6c2704c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4285.3994, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(400):\n",
        "  preds = model(inputs)\n",
        "  loss = mse(target,preds)\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w-=w.grad * learning_rate\n",
        "    b-=b.grad * learning_rate\n",
        "\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  print(f'epoch {i} loss {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09_FdAsfmUG7",
        "outputId": "03b34743-68f2-44a1-a5d9-8e7b16a825cf"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 loss 4285.3994140625\n",
            "epoch 1 loss 3134.844482421875\n",
            "epoch 2 loss 2356.428955078125\n",
            "epoch 3 loss 1828.832275390625\n",
            "epoch 4 loss 1470.2969970703125\n",
            "epoch 5 loss 1225.7271728515625\n",
            "epoch 6 loss 1057.9949951171875\n",
            "epoch 7 loss 942.0789184570312\n",
            "epoch 8 loss 861.1171875\n",
            "epoch 9 loss 803.7468872070312\n",
            "epoch 10 loss 762.3097534179688\n",
            "epoch 11 loss 731.6447143554688\n",
            "epoch 12 loss 708.2734985351562\n",
            "epoch 13 loss 689.8514404296875\n",
            "epoch 14 loss 674.7984619140625\n",
            "epoch 15 loss 662.0487060546875\n",
            "epoch 16 loss 650.8843994140625\n",
            "epoch 17 loss 640.8206787109375\n",
            "epoch 18 loss 631.5304565429688\n",
            "epoch 19 loss 622.793212890625\n",
            "epoch 20 loss 614.4598999023438\n",
            "epoch 21 loss 606.4297485351562\n",
            "epoch 22 loss 598.6337890625\n",
            "epoch 23 loss 591.0259399414062\n",
            "epoch 24 loss 583.5748291015625\n",
            "epoch 25 loss 576.2584228515625\n",
            "epoch 26 loss 569.0614624023438\n",
            "epoch 27 loss 561.9738159179688\n",
            "epoch 28 loss 554.9878540039062\n",
            "epoch 29 loss 548.0985107421875\n",
            "epoch 30 loss 541.3013916015625\n",
            "epoch 31 loss 534.5938720703125\n",
            "epoch 32 loss 527.9733276367188\n",
            "epoch 33 loss 521.4378662109375\n",
            "epoch 34 loss 514.9859619140625\n",
            "epoch 35 loss 508.6162109375\n",
            "epoch 36 loss 502.326904296875\n",
            "epoch 37 loss 496.11749267578125\n",
            "epoch 38 loss 489.9862365722656\n",
            "epoch 39 loss 483.9324645996094\n",
            "epoch 40 loss 477.955078125\n",
            "epoch 41 loss 472.052978515625\n",
            "epoch 42 loss 466.22528076171875\n",
            "epoch 43 loss 460.47100830078125\n",
            "epoch 44 loss 454.78912353515625\n",
            "epoch 45 loss 449.1788024902344\n",
            "epoch 46 loss 443.63916015625\n",
            "epoch 47 loss 438.1693420410156\n",
            "epoch 48 loss 432.768310546875\n",
            "epoch 49 loss 427.43524169921875\n",
            "epoch 50 loss 422.16943359375\n",
            "epoch 51 loss 416.9696350097656\n",
            "epoch 52 loss 411.83544921875\n",
            "epoch 53 loss 406.7659912109375\n",
            "epoch 54 loss 401.7602844238281\n",
            "epoch 55 loss 396.81756591796875\n",
            "epoch 56 loss 391.93701171875\n",
            "epoch 57 loss 387.11798095703125\n",
            "epoch 58 loss 382.3596496582031\n",
            "epoch 59 loss 377.66107177734375\n",
            "epoch 60 loss 373.02166748046875\n",
            "epoch 61 loss 368.4405822753906\n",
            "epoch 62 loss 363.91717529296875\n",
            "epoch 63 loss 359.45068359375\n",
            "epoch 64 loss 355.0403137207031\n",
            "epoch 65 loss 350.6855163574219\n",
            "epoch 66 loss 346.38543701171875\n",
            "epoch 67 loss 342.13970947265625\n",
            "epoch 68 loss 337.94720458984375\n",
            "epoch 69 loss 333.80743408203125\n",
            "epoch 70 loss 329.71966552734375\n",
            "epoch 71 loss 325.6834411621094\n",
            "epoch 72 loss 321.69793701171875\n",
            "epoch 73 loss 317.7626037597656\n",
            "epoch 74 loss 313.8765563964844\n",
            "epoch 75 loss 310.03961181640625\n",
            "epoch 76 loss 306.2508850097656\n",
            "epoch 77 loss 302.5097961425781\n",
            "epoch 78 loss 298.815673828125\n",
            "epoch 79 loss 295.1680603027344\n",
            "epoch 80 loss 291.5663757324219\n",
            "epoch 81 loss 288.0098571777344\n",
            "epoch 82 loss 284.4980163574219\n",
            "epoch 83 loss 281.03045654296875\n",
            "epoch 84 loss 277.60626220703125\n",
            "epoch 85 loss 274.2253723144531\n",
            "epoch 86 loss 270.8869323730469\n",
            "epoch 87 loss 267.59033203125\n",
            "epoch 88 loss 264.3353271484375\n",
            "epoch 89 loss 261.1211242675781\n",
            "epoch 90 loss 257.9473571777344\n",
            "epoch 91 loss 254.81338500976562\n",
            "epoch 92 loss 251.71890258789062\n",
            "epoch 93 loss 248.6632843017578\n",
            "epoch 94 loss 245.6458740234375\n",
            "epoch 95 loss 242.6665496826172\n",
            "epoch 96 loss 239.72470092773438\n",
            "epoch 97 loss 236.8197021484375\n",
            "epoch 98 loss 233.95126342773438\n",
            "epoch 99 loss 231.1186981201172\n",
            "epoch 100 loss 228.322021484375\n",
            "epoch 101 loss 225.5601806640625\n",
            "epoch 102 loss 222.83316040039062\n",
            "epoch 103 loss 220.14041137695312\n",
            "epoch 104 loss 217.4813232421875\n",
            "epoch 105 loss 214.8557586669922\n",
            "epoch 106 loss 212.26309204101562\n",
            "epoch 107 loss 209.7030029296875\n",
            "epoch 108 loss 207.1749725341797\n",
            "epoch 109 loss 204.67880249023438\n",
            "epoch 110 loss 202.21395874023438\n",
            "epoch 111 loss 199.78004455566406\n",
            "epoch 112 loss 197.37657165527344\n",
            "epoch 113 loss 195.00343322753906\n",
            "epoch 114 loss 192.66000366210938\n",
            "epoch 115 loss 190.3459930419922\n",
            "epoch 116 loss 188.0609893798828\n",
            "epoch 117 loss 185.80467224121094\n",
            "epoch 118 loss 183.57662963867188\n",
            "epoch 119 loss 181.3765869140625\n",
            "epoch 120 loss 179.20407104492188\n",
            "epoch 121 loss 177.05880737304688\n",
            "epoch 122 loss 174.94056701660156\n",
            "epoch 123 loss 172.8488311767578\n",
            "epoch 124 loss 170.7831573486328\n",
            "epoch 125 loss 168.74365234375\n",
            "epoch 126 loss 166.72958374023438\n",
            "epoch 127 loss 164.74081420898438\n",
            "epoch 128 loss 162.77694702148438\n",
            "epoch 129 loss 160.83767700195312\n",
            "epoch 130 loss 158.92274475097656\n",
            "epoch 131 loss 157.03182983398438\n",
            "epoch 132 loss 155.16461181640625\n",
            "epoch 133 loss 153.32070922851562\n",
            "epoch 134 loss 151.49998474121094\n",
            "epoch 135 loss 149.70204162597656\n",
            "epoch 136 loss 147.92672729492188\n",
            "epoch 137 loss 146.1735382080078\n",
            "epoch 138 loss 144.44235229492188\n",
            "epoch 139 loss 142.7328338623047\n",
            "epoch 140 loss 141.04478454589844\n",
            "epoch 141 loss 139.3778076171875\n",
            "epoch 142 loss 137.731689453125\n",
            "epoch 143 loss 136.10609436035156\n",
            "epoch 144 loss 134.50100708007812\n",
            "epoch 145 loss 132.91598510742188\n",
            "epoch 146 loss 131.35086059570312\n",
            "epoch 147 loss 129.8052520751953\n",
            "epoch 148 loss 128.2789306640625\n",
            "epoch 149 loss 126.77183532714844\n",
            "epoch 150 loss 125.28355407714844\n",
            "epoch 151 loss 123.8138198852539\n",
            "epoch 152 loss 122.36256408691406\n",
            "epoch 153 loss 120.929443359375\n",
            "epoch 154 loss 119.51420593261719\n",
            "epoch 155 loss 118.1167221069336\n",
            "epoch 156 loss 116.73665618896484\n",
            "epoch 157 loss 115.37386322021484\n",
            "epoch 158 loss 114.0280990600586\n",
            "epoch 159 loss 112.69917297363281\n",
            "epoch 160 loss 111.3868408203125\n",
            "epoch 161 loss 110.0909194946289\n",
            "epoch 162 loss 108.8111801147461\n",
            "epoch 163 loss 107.54754638671875\n",
            "epoch 164 loss 106.29959869384766\n",
            "epoch 165 loss 105.06727600097656\n",
            "epoch 166 loss 103.85028076171875\n",
            "epoch 167 loss 102.64848327636719\n",
            "epoch 168 loss 101.4617919921875\n",
            "epoch 169 loss 100.2898941040039\n",
            "epoch 170 loss 99.13252258300781\n",
            "epoch 171 loss 97.98963928222656\n",
            "epoch 172 loss 96.86109924316406\n",
            "epoch 173 loss 95.7465591430664\n",
            "epoch 174 loss 94.6459732055664\n",
            "epoch 175 loss 93.55909729003906\n",
            "epoch 176 loss 92.48576354980469\n",
            "epoch 177 loss 91.42586517333984\n",
            "epoch 178 loss 90.37919616699219\n",
            "epoch 179 loss 89.3454818725586\n",
            "epoch 180 loss 88.32474517822266\n",
            "epoch 181 loss 87.3167724609375\n",
            "epoch 182 loss 86.32125854492188\n",
            "epoch 183 loss 85.33821105957031\n",
            "epoch 184 loss 84.36746978759766\n",
            "epoch 185 loss 83.40872192382812\n",
            "epoch 186 loss 82.46192932128906\n",
            "epoch 187 loss 81.5269775390625\n",
            "epoch 188 loss 80.6036605834961\n",
            "epoch 189 loss 79.69180297851562\n",
            "epoch 190 loss 78.79130554199219\n",
            "epoch 191 loss 77.90205383300781\n",
            "epoch 192 loss 77.02384948730469\n",
            "epoch 193 loss 76.15660095214844\n",
            "epoch 194 loss 75.30013275146484\n",
            "epoch 195 loss 74.454345703125\n",
            "epoch 196 loss 73.61895751953125\n",
            "epoch 197 loss 72.79410552978516\n",
            "epoch 198 loss 71.9793701171875\n",
            "epoch 199 loss 71.17485046386719\n",
            "epoch 200 loss 70.38032531738281\n",
            "epoch 201 loss 69.59574890136719\n",
            "epoch 202 loss 68.82081604003906\n",
            "epoch 203 loss 68.05555725097656\n",
            "epoch 204 loss 67.29978942871094\n",
            "epoch 205 loss 66.55335998535156\n",
            "epoch 206 loss 65.8163070678711\n",
            "epoch 207 loss 65.08834075927734\n",
            "epoch 208 loss 64.36943817138672\n",
            "epoch 209 loss 63.65946578979492\n",
            "epoch 210 loss 62.95823287963867\n",
            "epoch 211 loss 62.265785217285156\n",
            "epoch 212 loss 61.58185577392578\n",
            "epoch 213 loss 60.906471252441406\n",
            "epoch 214 loss 60.23944854736328\n",
            "epoch 215 loss 59.58066940307617\n",
            "epoch 216 loss 58.930023193359375\n",
            "epoch 217 loss 58.287513732910156\n",
            "epoch 218 loss 57.652984619140625\n",
            "epoch 219 loss 57.02619171142578\n",
            "epoch 220 loss 56.4072380065918\n",
            "epoch 221 loss 55.79594802856445\n",
            "epoch 222 loss 55.19225311279297\n",
            "epoch 223 loss 54.59595489501953\n",
            "epoch 224 loss 54.0070915222168\n",
            "epoch 225 loss 53.42549514770508\n",
            "epoch 226 loss 52.8510627746582\n",
            "epoch 227 loss 52.28376007080078\n",
            "epoch 228 loss 51.723487854003906\n",
            "epoch 229 loss 51.17011642456055\n",
            "epoch 230 loss 50.62361526489258\n",
            "epoch 231 loss 50.08386993408203\n",
            "epoch 232 loss 49.55073928833008\n",
            "epoch 233 loss 49.0242805480957\n",
            "epoch 234 loss 48.50423049926758\n",
            "epoch 235 loss 47.99065399169922\n",
            "epoch 236 loss 47.48334503173828\n",
            "epoch 237 loss 46.98235321044922\n",
            "epoch 238 loss 46.48754119873047\n",
            "epoch 239 loss 45.998809814453125\n",
            "epoch 240 loss 45.516117095947266\n",
            "epoch 241 loss 45.03937911987305\n",
            "epoch 242 loss 44.56850051879883\n",
            "epoch 243 loss 44.10344314575195\n",
            "epoch 244 loss 43.64409637451172\n",
            "epoch 245 loss 43.19038772583008\n",
            "epoch 246 loss 42.742271423339844\n",
            "epoch 247 loss 42.299705505371094\n",
            "epoch 248 loss 41.8625602722168\n",
            "epoch 249 loss 41.430763244628906\n",
            "epoch 250 loss 41.00428009033203\n",
            "epoch 251 loss 40.58307647705078\n",
            "epoch 252 loss 40.16703414916992\n",
            "epoch 253 loss 39.756065368652344\n",
            "epoch 254 loss 39.35017013549805\n",
            "epoch 255 loss 38.94930648803711\n",
            "epoch 256 loss 38.553279876708984\n",
            "epoch 257 loss 38.16215515136719\n",
            "epoch 258 loss 37.77582550048828\n",
            "epoch 259 loss 37.39421844482422\n",
            "epoch 260 loss 37.017311096191406\n",
            "epoch 261 loss 36.6450080871582\n",
            "epoch 262 loss 36.277259826660156\n",
            "epoch 263 loss 35.91401290893555\n",
            "epoch 264 loss 35.55522918701172\n",
            "epoch 265 loss 35.200828552246094\n",
            "epoch 266 loss 34.85078811645508\n",
            "epoch 267 loss 34.50501251220703\n",
            "epoch 268 loss 34.16346740722656\n",
            "epoch 269 loss 33.82606506347656\n",
            "epoch 270 loss 33.49282455444336\n",
            "epoch 271 loss 33.16360092163086\n",
            "epoch 272 loss 32.838462829589844\n",
            "epoch 273 loss 32.51725387573242\n",
            "epoch 274 loss 32.19997024536133\n",
            "epoch 275 loss 31.886554718017578\n",
            "epoch 276 loss 31.576946258544922\n",
            "epoch 277 loss 31.271154403686523\n",
            "epoch 278 loss 30.969036102294922\n",
            "epoch 279 loss 30.67062759399414\n",
            "epoch 280 loss 30.375829696655273\n",
            "epoch 281 loss 30.08462905883789\n",
            "epoch 282 loss 29.796955108642578\n",
            "epoch 283 loss 29.5128173828125\n",
            "epoch 284 loss 29.232097625732422\n",
            "epoch 285 loss 28.954776763916016\n",
            "epoch 286 loss 28.68088722229004\n",
            "epoch 287 loss 28.410247802734375\n",
            "epoch 288 loss 28.142929077148438\n",
            "epoch 289 loss 27.878849029541016\n",
            "epoch 290 loss 27.61794662475586\n",
            "epoch 291 loss 27.360254287719727\n",
            "epoch 292 loss 27.105640411376953\n",
            "epoch 293 loss 26.854122161865234\n",
            "epoch 294 loss 26.605632781982422\n",
            "epoch 295 loss 26.36016845703125\n",
            "epoch 296 loss 26.11769676208496\n",
            "epoch 297 loss 25.878097534179688\n",
            "epoch 298 loss 25.641427993774414\n",
            "epoch 299 loss 25.407594680786133\n",
            "epoch 300 loss 25.176620483398438\n",
            "epoch 301 loss 24.948406219482422\n",
            "epoch 302 loss 24.722896575927734\n",
            "epoch 303 loss 24.50018882751465\n",
            "epoch 304 loss 24.280120849609375\n",
            "epoch 305 loss 24.06272315979004\n",
            "epoch 306 loss 23.847919464111328\n",
            "epoch 307 loss 23.635690689086914\n",
            "epoch 308 loss 23.426029205322266\n",
            "epoch 309 loss 23.21888542175293\n",
            "epoch 310 loss 23.014211654663086\n",
            "epoch 311 loss 22.812036514282227\n",
            "epoch 312 loss 22.612266540527344\n",
            "epoch 313 loss 22.41489028930664\n",
            "epoch 314 loss 22.219867706298828\n",
            "epoch 315 loss 22.027185440063477\n",
            "epoch 316 loss 21.836807250976562\n",
            "epoch 317 loss 21.648714065551758\n",
            "epoch 318 loss 21.462871551513672\n",
            "epoch 319 loss 21.27921485900879\n",
            "epoch 320 loss 21.097806930541992\n",
            "epoch 321 loss 20.918529510498047\n",
            "epoch 322 loss 20.741374969482422\n",
            "epoch 323 loss 20.566360473632812\n",
            "epoch 324 loss 20.3934326171875\n",
            "epoch 325 loss 20.222530364990234\n",
            "epoch 326 loss 20.053714752197266\n",
            "epoch 327 loss 19.886871337890625\n",
            "epoch 328 loss 19.722007751464844\n",
            "epoch 329 loss 19.559114456176758\n",
            "epoch 330 loss 19.398143768310547\n",
            "epoch 331 loss 19.239078521728516\n",
            "epoch 332 loss 19.0819091796875\n",
            "epoch 333 loss 18.926593780517578\n",
            "epoch 334 loss 18.773113250732422\n",
            "epoch 335 loss 18.621479034423828\n",
            "epoch 336 loss 18.471586227416992\n",
            "epoch 337 loss 18.323490142822266\n",
            "epoch 338 loss 18.177141189575195\n",
            "epoch 339 loss 18.03252601623535\n",
            "epoch 340 loss 17.889598846435547\n",
            "epoch 341 loss 17.748376846313477\n",
            "epoch 342 loss 17.60879898071289\n",
            "epoch 343 loss 17.47089385986328\n",
            "epoch 344 loss 17.334562301635742\n",
            "epoch 345 loss 17.199859619140625\n",
            "epoch 346 loss 17.066730499267578\n",
            "epoch 347 loss 16.9351749420166\n",
            "epoch 348 loss 16.805143356323242\n",
            "epoch 349 loss 16.676624298095703\n",
            "epoch 350 loss 16.549617767333984\n",
            "epoch 351 loss 16.424114227294922\n",
            "epoch 352 loss 16.300079345703125\n",
            "epoch 353 loss 16.177474975585938\n",
            "epoch 354 loss 16.056283950805664\n",
            "epoch 355 loss 15.936544418334961\n",
            "epoch 356 loss 15.818178176879883\n",
            "epoch 357 loss 15.70118522644043\n",
            "epoch 358 loss 15.585561752319336\n",
            "epoch 359 loss 15.47126579284668\n",
            "epoch 360 loss 15.358302116394043\n",
            "epoch 361 loss 15.246661186218262\n",
            "epoch 362 loss 15.13630199432373\n",
            "epoch 363 loss 15.027227401733398\n",
            "epoch 364 loss 14.919389724731445\n",
            "epoch 365 loss 14.812812805175781\n",
            "epoch 366 loss 14.707494735717773\n",
            "epoch 367 loss 14.6033296585083\n",
            "epoch 368 loss 14.500429153442383\n",
            "epoch 369 loss 14.398681640625\n",
            "epoch 370 loss 14.29809856414795\n",
            "epoch 371 loss 14.198684692382812\n",
            "epoch 372 loss 14.100377082824707\n",
            "epoch 373 loss 14.00323486328125\n",
            "epoch 374 loss 13.907185554504395\n",
            "epoch 375 loss 13.81225299835205\n",
            "epoch 376 loss 13.718404769897461\n",
            "epoch 377 loss 13.625608444213867\n",
            "epoch 378 loss 13.533899307250977\n",
            "epoch 379 loss 13.44318675994873\n",
            "epoch 380 loss 13.353550910949707\n",
            "epoch 381 loss 13.264928817749023\n",
            "epoch 382 loss 13.177309036254883\n",
            "epoch 383 loss 13.090667724609375\n",
            "epoch 384 loss 13.005020141601562\n",
            "epoch 385 loss 12.92035961151123\n",
            "epoch 386 loss 12.836618423461914\n",
            "epoch 387 loss 12.753854751586914\n",
            "epoch 388 loss 12.671992301940918\n",
            "epoch 389 loss 12.591090202331543\n",
            "epoch 390 loss 12.511085510253906\n",
            "epoch 391 loss 12.431958198547363\n",
            "epoch 392 loss 12.35374641418457\n",
            "epoch 393 loss 12.276407241821289\n",
            "epoch 394 loss 12.199941635131836\n",
            "epoch 395 loss 12.124320030212402\n",
            "epoch 396 loss 12.049540519714355\n",
            "epoch 397 loss 11.975598335266113\n",
            "epoch 398 loss 11.902475357055664\n",
            "epoch 399 loss 11.830204010009766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "loss = mse(target,preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP1a8omlnAAh",
        "outputId": "a3aefd6e-8612-4a99-a3de-9a69c7306a30"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.7587, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "sqrt(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTyu1XlBnUzj",
        "outputId": "1145f438-7710-49e0-8044-9bb8690772e1"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.4290958667613745"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjznBNySntfQ",
        "outputId": "2e941dc9-5471-46ed-88f8-5bbc1fe221a0"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.4285,  67.5333],\n",
              "        [ 80.9564, 103.1001],\n",
              "        [122.7215, 107.1478],\n",
              "        [ 16.7013,  40.0053],\n",
              "        [102.3057, 123.6064]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUbU99e7nuzp",
        "outputId": "b5ebe099-e477-4516-dcd6-12696b9bcee8"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 113.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oyHfrpQEnwCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}