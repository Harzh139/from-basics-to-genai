{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30086694",
        "outputId": "1c197e40-bd9c-4e9b-93f2-9b4e15131b05"
      },
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38ebebc4",
        "outputId": "fa14ca77-afea-4ffd-b73e-ef7ddae0e131"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "# Synthetic dataset of simple code snippets and their outputs\n",
        "data = [\n",
        "    (\"print('hello')\", \"hello\"),\n",
        "    (\"x = 5\\nprint(x)\", \"5\"),\n",
        "    (\"a = 10\\nb = 20\\nprint(a + b)\", \"30\"),\n",
        "    (\"for i in range(3):\\n  print(i)\", \"0\\n1\\n2\"),\n",
        "    (\"def greet(name):\\n  return f'Hi, {name}'\\nprint(greet('Alice'))\", \"Hi, Alice\")\n",
        "]\n",
        "\n",
        "# Separate input and output sequences\n",
        "input_sequences, output_sequences = zip(*data)\n",
        "\n",
        "print(\"Input Sequences:\")\n",
        "for seq in input_sequences:\n",
        "    print(seq)\n",
        "\n",
        "print(\"\\nOutput Sequences:\")\n",
        "for seq in output_sequences:\n",
        "    print(seq)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequences:\n",
            "print('hello')\n",
            "x = 5\n",
            "print(x)\n",
            "a = 10\n",
            "b = 20\n",
            "print(a + b)\n",
            "for i in range(3):\n",
            "  print(i)\n",
            "def greet(name):\n",
            "  return f'Hi, {name}'\n",
            "print(greet('Alice'))\n",
            "\n",
            "Output Sequences:\n",
            "hello\n",
            "5\n",
            "30\n",
            "0\n",
            "1\n",
            "2\n",
            "Hi, Alice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a6c9a28",
        "outputId": "9bfc55bb-5e20-49a6-c460-59f7b99fb4c3"
      },
      "source": [
        "# Tokenize and build vocabulary for input sequences\n",
        "input_tokens = [list(seq) for seq in input_sequences]\n",
        "input_vocab = Counter(itertools.chain(*input_tokens))\n",
        "input_stoi = {token: i + 2 for i, (token, count) in enumerate(input_vocab.most_common())}\n",
        "input_stoi['<pad>'] = 0\n",
        "input_stoi['<unk>'] = 1\n",
        "input_itos = {i: token for token, i in input_stoi.items()}\n",
        "\n",
        "# Tokenize and build vocabulary for output sequences\n",
        "output_tokens = [list(seq) for seq in output_sequences]\n",
        "output_vocab = Counter(itertools.chain(*output_tokens))\n",
        "output_stoi = {token: i + 2 for i, (token, count) in enumerate(output_vocab.most_common())}\n",
        "output_stoi['<pad>'] = 0\n",
        "output_stoi['<unk>'] = 1\n",
        "output_stoi['<sos>'] = len(output_stoi) # Start of sequence token\n",
        "output_stoi['<eos>'] = len(output_stoi) # End of sequence token\n",
        "output_itos = {i: token for token, i in output_stoi.items()}\n",
        "\n",
        "print(\"Input Vocabulary Size:\", len(input_stoi))\n",
        "print(\"Output Vocabulary Size:\", len(output_stoi))\n",
        "print(\"\\nInput stoi:\", input_stoi)\n",
        "print(\"\\nOutput stoi:\", output_stoi)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Vocabulary Size: 38\n",
            "Output Vocabulary Size: 20\n",
            "\n",
            "Input stoi: {' ': 2, 'r': 3, 'e': 4, 'i': 5, 'n': 6, 't': 7, '(': 8, ')': 9, \"'\": 10, '\\n': 11, 'p': 12, 'a': 13, 'l': 14, '=': 15, 'f': 16, 'g': 17, 'o': 18, 'x': 19, '0': 20, 'b': 21, ':': 22, 'm': 23, 'h': 24, '5': 25, '1': 26, '2': 27, '+': 28, '3': 29, 'd': 30, 'u': 31, 'H': 32, ',': 33, '{': 34, '}': 35, 'A': 36, 'c': 37, '<pad>': 0, '<unk>': 1}\n",
            "\n",
            "Output stoi: {'l': 2, 'e': 3, '0': 4, '\\n': 5, 'i': 6, 'h': 7, 'o': 8, '5': 9, '3': 10, '1': 11, '2': 12, 'H': 13, ',': 14, ' ': 15, 'A': 16, 'c': 17, '<pad>': 0, '<unk>': 1, '<sos>': 18, '<eos>': 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9851b62e",
        "outputId": "dd6ed5ac-bed5-424e-a90c-800d34303f57"
      },
      "source": [
        "# Convert tokens to numerical sequences\n",
        "input_numerical_sequences = [[input_stoi.get(token, input_stoi['<unk>']) for token in seq] for seq in input_tokens]\n",
        "output_numerical_sequences = [[output_stoi.get(token, output_stoi['<unk>']) for token in seq] for seq in output_tokens]\n",
        "\n",
        "# Add <sos> and <eos> tokens to output sequences\n",
        "output_numerical_sequences = [[output_stoi['<sos>']] + seq + [output_stoi['<eos>']] for seq in output_numerical_sequences]\n",
        "\n",
        "# Determine maximum sequence lengths for padding\n",
        "max_input_length = max(len(seq) for seq in input_numerical_sequences)\n",
        "max_output_length = max(len(seq) for seq in output_numerical_sequences)\n",
        "\n",
        "# Pad sequences\n",
        "padded_input_sequences = [seq + [input_stoi['<pad>']] * (max_input_length - len(seq)) for seq in input_numerical_sequences]\n",
        "padded_output_sequences = [seq + [output_stoi['<pad>']] * (max_output_length - len(seq)) for seq in output_numerical_sequences]\n",
        "\n",
        "print(\"Padded Input Sequences (first):\", padded_input_sequences[0])\n",
        "print(\"Padded Output Sequences (first):\", padded_output_sequences[0])\n",
        "print(\"\\nMax Input Length:\", max_input_length)\n",
        "print(\"Max Output Length:\", max_output_length)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded Input Sequences (first): [12, 3, 5, 6, 7, 8, 10, 24, 4, 14, 14, 18, 10, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Padded Output Sequences (first): [18, 7, 3, 2, 2, 8, 19, 0, 0, 0, 0]\n",
            "\n",
            "Max Input Length: 61\n",
            "Max Output Length: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a691949",
        "outputId": "e45be239-d6f0-46b3-81c8-9ab945bca38e"
      },
      "source": [
        "# Convert padded sequences to PyTorch tensors\n",
        "input_tensors = torch.LongTensor(padded_input_sequences)\n",
        "output_tensors = torch.LongTensor(padded_output_sequences)\n",
        "\n",
        "# For teacher forcing, the target is the output sequence shifted by one position\n",
        "target_tensors = output_tensors[:, 1:] # Exclude the <sos> token\n",
        "\n",
        "print(\"Input Tensors shape:\", input_tensors.shape)\n",
        "print(\"Output Tensors shape:\", output_tensors.shape)\n",
        "print(\"Target Tensors shape:\", target_tensors.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensors shape: torch.Size([5, 61])\n",
            "Output Tensors shape: torch.Size([5, 11])\n",
            "Target Tensors shape: torch.Size([5, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e3b520f"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: (sequence_length, batch_size)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded shape: (sequence_length, batch_size, emb_dim)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs shape: (sequence_length, batch_size, hid_dim * num_directions)\n",
        "        # hidden shape: (n_layers * num_directions, batch_size, hid_dim)\n",
        "        # cell shape: (n_layers * num_directions, batch_size, hid_dim)\n",
        "\n",
        "        # We only need the final hidden and cell states for the decoder\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim) # Adjust size for concat\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input shape: (batch_size) -> needs unsqueezing\n",
        "        # hidden shape: (n_layers, batch_size, hid_dim)\n",
        "        # cell shape: (n_layers, batch_size, hid_dim)\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        # input shape: (1, batch_size)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded shape: (1, batch_size, emb_dim)\n",
        "\n",
        "        # Context vector from encoder (last hidden state)\n",
        "        # We will concatenate the embedded input with the context vector\n",
        "        context = hidden[-1, :, :].unsqueeze(0) # Using the last layer's hidden state as context\n",
        "        # context shape: (1, batch_size, hid_dim)\n",
        "\n",
        "\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        # rnn_input shape: (1, batch_size, emb_dim + hid_dim)\n",
        "\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        # output shape: (1, batch_size, hid_dim * num_directions)\n",
        "        # hidden shape: (n_layers * num_directions, batch_size, hid_dim)\n",
        "        # cell shape: (n_layers * num_directions, batch_size, hid_dim)\n",
        "\n",
        "        # output is from the top RNN layer\n",
        "        # hidden and cell are from all layers\n",
        "\n",
        "        # For the linear layer, we need to concatenate the output, embedded input, and context\n",
        "        output = output.squeeze(0) # Remove the sequence length dimension (which is 1)\n",
        "        embedded = embedded.squeeze(0) # Remove the sequence length dimension (which is 1)\n",
        "        context = context.squeeze(0) # Remove the sequence length dimension (which is 1)\n",
        "\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, embedded, context), dim=1))\n",
        "        # prediction shape: (batch_size, output_dim)\n",
        "\n",
        "\n",
        "        return prediction, hidden, cell\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fe71842",
        "outputId": "958d6ac8-592c-4d28-eefa-1e3a1e37001e"
      },
      "source": [
        "import random\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Number of layers of encoder and decoder must be equal!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src shape: (sequence_length, batch_size)\n",
        "        # trg shape: (sequence_length, batch_size)\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encoder outputs the final hidden and cell states\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is the <sos> tokens\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # Insert input token embedding, previous hidden and cell states\n",
        "            # receive output prediction and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            # Place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # Get the highest predicted token from the output\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # If teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            input = trg[t, :] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Instantiate models and set up training parameters\n",
        "INPUT_DIM = len(input_stoi)\n",
        "OUTPUT_DIM = len(output_stoi)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "TRG_PAD_IDX = output_stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
        "\n",
        "# Simple DataLoader (for demonstration purposes, a proper DataLoader should be used)\n",
        "# Assuming input_tensors and output_tensors are already created and on the correct device\n",
        "# In a real scenario, you would create a Dataset and DataLoader\n",
        "# For this simple case, we'll treat the entire dataset as a single batch for demonstration\n",
        "train_input = input_tensors.T.to(device) # Transpose to get (sequence_length, batch_size)\n",
        "train_output = output_tensors.T.to(device) # Transpose to get (sequence_length, batch_size)\n",
        "\n",
        "# Training loop (one epoch for demonstration)\n",
        "def train(model, src, trg, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(src, trg)\n",
        "\n",
        "    # trg shape: (trg_len, batch_size)\n",
        "    # output shape: (trg_len, batch_size, output_dim)\n",
        "\n",
        "    output_dim = output.shape[-1]\n",
        "\n",
        "    # Reshape for criterion (batch_size * trg_len, output_dim)\n",
        "    # Target needs to be (batch_size * trg_len)\n",
        "    output = output[1:].view(-1, output_dim) # Exclude <sos> token output\n",
        "    trg = trg[1:].contiguous().view(-1)     # Exclude <sos> token target\n",
        "\n",
        "    loss = criterion(output, trg)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# Training (demonstration with a single \"batch\")\n",
        "CLIP = 1\n",
        "loss = train(model, train_input, train_output, optimizer, criterion, CLIP)\n",
        "\n",
        "print(f'Loss after one training step: {loss:.4f}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after one training step: 3.0091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViXokvmE_H6k",
        "outputId": "e35616f7-f9a4-4f94-d2c7-090f2b45e7ab"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def evaluate(model, src, trg, criterion, output_itos, trg_pad_idx, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    all_predicted_tokens = []\n",
        "    all_target_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate through the data (treating each sequence as a batch for simplicity)\n",
        "        for i in range(src.shape[1]):\n",
        "            single_src = src[:, i].unsqueeze(1) # Shape: (seq_len, 1)\n",
        "            single_trg = trg[:, i].unsqueeze(1) # Shape: (seq_len, 1)\n",
        "\n",
        "            # Forward pass to get encoder hidden and cell states\n",
        "            hidden, cell = model.encoder(single_src)\n",
        "\n",
        "            # Decoder's first input is the <sos> token\n",
        "            input = single_trg[0, :] # Shape: (1)\n",
        "\n",
        "            # Tensor to store decoder outputs for this sequence\n",
        "            # Initialize with a size based on max_output_length to avoid index errors\n",
        "            outputs = torch.zeros(max_output_length, model.decoder.output_dim).to(device)\n",
        "            predicted_tokens = []\n",
        "\n",
        "            # Greedy decoding\n",
        "            for t in range(1, max_output_length): # Iterate up to max_output_length\n",
        "                output, hidden, cell = model.decoder(input, hidden, cell)\n",
        "\n",
        "                # Store the output for loss calculation\n",
        "                outputs[t] = output.squeeze(0)\n",
        "\n",
        "                # Get the highest predicted token\n",
        "                top1 = output.argmax(1)\n",
        "\n",
        "                # Use the predicted token as the next input\n",
        "                input = top1\n",
        "\n",
        "                # Convert predicted token index back to token and store\n",
        "                predicted_token = output_itos[top1.item()]\n",
        "                if predicted_token == '<eos>':\n",
        "                    break # Stop decoding if <eos> is predicted\n",
        "                if predicted_token != '<pad>': # Don't include padding in prediction\n",
        "                     predicted_tokens.append(predicted_token)\n",
        "\n",
        "\n",
        "            # Calculate loss for this sequence\n",
        "            # Only consider the predicted tokens up to the point decoding stopped or max length reached\n",
        "            actual_len = min(t + 1, single_trg.shape[0]) # Length of target sequence including <eos>\n",
        "            predicted_len = len(predicted_tokens) + 1 # Length of predicted sequence including <sos> and potential <eos>\n",
        "\n",
        "            # Ensure that the output tensor used for loss calculation matches the length of the target tensor slice\n",
        "            loss = criterion(outputs[1:actual_len].view(-1, outputs.shape[-1]), single_trg[1:actual_len].contiguous().view(-1))\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "            # Convert target sequence to tokens for BLEU calculation\n",
        "            # Exclude <sos> and <eos> for BLEU calculation\n",
        "            target_tokens = [output_itos[token.item()] for token in single_trg[1:] if output_itos[token.item()] != '<eos>' and token.item() != trg_pad_idx]\n",
        "\n",
        "            # Convert token lists to tuples of strings for nltk\n",
        "            all_predicted_tokens.append(tuple(predicted_tokens))\n",
        "            all_target_tokens.append(tuple(target_tokens))\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = epoch_loss / src.shape[1]\n",
        "\n",
        "    # Calculate BLEU score (corpus BLEU is more robust for small datasets)\n",
        "    # Need to format for sentence_bleu or corpus_bleu\n",
        "    # reference_corpus = list of list of tokens (each inner list is a reference)\n",
        "    # candidate_corpus = list of tokens\n",
        "    reference_corpus = [[list(tokens)] for tokens in all_target_tokens] # sentence_bleu expects a list of references\n",
        "    candidate_corpus = [list(tokens) for tokens in all_predicted_tokens]\n",
        "\n",
        "    # Calculate sentence BLEU for each example and average\n",
        "    # Ensure references are lists of lists, candidates are lists\n",
        "    bleu_scores = [sentence_bleu([list(ref)], list(cand)) for ref, cand in zip(all_target_tokens, all_predicted_tokens)]\n",
        "    avg_bleu = np.mean(bleu_scores)\n",
        "\n",
        "\n",
        "    return avg_loss, avg_bleu\n",
        "\n",
        "# Call the evaluation function\n",
        "eval_loss, eval_bleu = evaluate(model, train_input, train_output, criterion, output_itos, TRG_PAD_IDX, device)\n",
        "\n",
        "print(f'Evaluation Loss: {eval_loss:.4f}')\n",
        "print(f'Evaluation BLEU Score: {eval_bleu:.4f}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 2.8140\n",
            "Evaluation BLEU Score: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6199319d",
        "outputId": "9fa0e0b9-8c82-42df-8d9f-e704908cd339"
      },
      "source": [
        "import torch\n",
        "\n",
        "def translate_sequence(sequence, model, input_stoi, output_itos, device, max_output_length=50):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Preprocess the input sequence\n",
        "        tokens = [input_stoi.get(token, input_stoi['<unk>']) for token in list(sequence)]\n",
        "        src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device) # Add batch dimension and move to device\n",
        "\n",
        "        # Pass through encoder\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # Initialize decoder input with <sos> token\n",
        "        input_tensor = torch.LongTensor([output_stoi['<sos>']]).to(device) # Shape: (1)\n",
        "\n",
        "        generated_indices = []\n",
        "\n",
        "        # Generate output sequence\n",
        "        for _ in range(max_output_length):\n",
        "            # Pass current input and previous hidden/cell states to decoder\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "\n",
        "            # Get the index of the predicted next token\n",
        "            predicted_token_index = output.argmax(1).item()\n",
        "            generated_indices.append(predicted_token_index)\n",
        "\n",
        "            # Stop if <eos> token is predicted\n",
        "            if predicted_token_index == output_stoi['<eos>']:\n",
        "                break\n",
        "\n",
        "            # Use the predicted token as the input for the next step\n",
        "            input_tensor = torch.LongTensor([predicted_token_index]).to(device)\n",
        "\n",
        "        # Convert generated indices back to tokens\n",
        "        generated_tokens = [output_itos[index] for index in generated_indices]\n",
        "\n",
        "        # Remove <sos> and <eos> tokens if present\n",
        "        if generated_tokens[0] == '<sos>':\n",
        "            generated_tokens = generated_tokens[1:]\n",
        "        if generated_tokens and generated_tokens[-1] == '<eos>':\n",
        "            generated_tokens = generated_tokens[:-1]\n",
        "\n",
        "    return ''.join(generated_tokens) # Join tokens back into a string\n",
        "\n",
        "# Demonstrate usage with an example input sequence\n",
        "example_input = \"print('hello')\"\n",
        "translated_output = translate_sequence(example_input, model, input_stoi, output_itos, device)\n",
        "\n",
        "print(f\"Input: {example_input}\")\n",
        "print(f\"Generated Output: {translated_output}\")\n",
        "\n",
        "example_input_2 = \"x = 5\\nprint(x)\"\n",
        "translated_output_2 = translate_sequence(example_input_2, model, input_stoi, output_itos, device)\n",
        "\n",
        "print(f\"\\nInput: {example_input_2}\")\n",
        "print(f\"Generated Output: {translated_output_2}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: print('hello')\n",
            "Generated Output: 01\n",
            "\n",
            "Input: x = 5\n",
            "print(x)\n",
            "Generated Output: 0101\n"
          ]
        }
      ]
    }
  ]
}