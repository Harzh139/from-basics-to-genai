{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff7cf450"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  \"\"\"\n",
        "  Computes scaled dot-product attention.\n",
        "\n",
        "  Args:\n",
        "    query: Tensor of shape (batch_size, num_heads, seq_len_q, dim_k).\n",
        "    key: Tensor of shape (batch_size, num_heads, seq_len_kv, dim_k).\n",
        "    value: Tensor of shape (batch_size, num_heads, seq_len_kv, dim_v).\n",
        "    mask: Optional mask tensor of shape (batch_size, 1, 1, seq_len_kv).\n",
        "\n",
        "  Returns:\n",
        "    Output tensor of shape (batch_size, num_heads, seq_len_q, dim_v).\n",
        "    Attention weights tensor of shape (batch_size, num_heads, seq_len_q, seq_len_kv).\n",
        "  \"\"\"\n",
        "  matmul_qk = torch.matmul(query, key.transpose(-2, -1)) # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # Scale matmul_qk\n",
        "  dim_k = torch.tensor(key.shape[-1], dtype=torch.float32)\n",
        "  scaled_attention_logits = matmul_qk / torch.sqrt(dim_k)\n",
        "\n",
        "  # Add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "\n",
        "  output = torch.matmul(attention_weights, value) # (..., seq_len_q, dim_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05440555"
      },
      "source": [
        "## Implement multi-head attention\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b85fdf2"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, v, k, q, mask):\n",
        "        batch_size = q.shape[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8e7ef7"
      },
      "source": [
        "## Create positional encoding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "304b6ab9"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c3ead5a"
      },
      "source": [
        "## Build the encoder layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92c5306b"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(rate)\n",
        "        self.dropout2 = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # Self-attention\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Add & Norm\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # Feed-forward\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Add & Norm\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e85e5a13"
      },
      "source": [
        "## Construct the decoder layer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "969859b9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Masked self-attention\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Encoder-decoder attention\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(rate)\n",
        "        self.dropout2 = nn.Dropout(rate)\n",
        "        self.dropout3 = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # Masked self-attention\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(x + attn1)  # Add & Norm\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # Encoder-decoder attention\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(out1 + attn2)  # Add & Norm\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # Feed-forward\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(out2 + ffn_output)  # Add & Norm\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ed25e32"
      },
      "source": [
        "## Assemble the transformer model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f22547f"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        self.pe_input = pe_input\n",
        "        self.pe_target = pe_target\n",
        "        self.rate = rate\n",
        "\n",
        "        self.embedding_input = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.embedding_target = nn.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=max(pe_input, pe_target))\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                                             for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                                             for _ in range(num_layers)])\n",
        "\n",
        "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, inp, tar, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        # inp.shape == (batch_size, seq_len_in)\n",
        "        # tar.shape == (batch_size, seq_len_out)\n",
        "        # enc_padding_mask.shape == (batch_size, 1, 1, seq_len_in)\n",
        "        # look_ahead_mask.shape == (batch_size, 1, seq_len_out, seq_len_out)\n",
        "        # dec_padding_mask.shape == (batch_size, 1, 1, seq_len_in)\n",
        "\n",
        "        # Encoder\n",
        "        inp = self.embedding_input(inp)  # (batch_size, seq_len_in, d_model)\n",
        "        inp += self.pos_encoding(inp)\n",
        "        enc_output = self.dropout(inp)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            enc_output = self.encoder_layers[i](enc_output, enc_padding_mask)\n",
        "\n",
        "        # Decoder\n",
        "        tar = self.embedding_target(tar)  # (batch_size, seq_len_out, d_model)\n",
        "        tar += self.pos_encoding(tar)\n",
        "        dec_output = self.dropout(tar)\n",
        "\n",
        "        attention_weights = {}\n",
        "        for i in range(self.num_layers):\n",
        "            dec_output, attn1, attn2 = self.decoder_layers[i](\n",
        "                dec_output, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = attn1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = attn2\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, seq_len_out, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdfe6fce"
      },
      "source": [
        "## Implement training and evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI6c0BnzkvDa"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define the loss function\n",
        "loss_object = nn.CrossEntropyLoss(ignore_index=0, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = torch.logical_not(torch.eq(real, 0)).float()\n",
        "    loss_ = loss_object(pred.transpose(-1, -2), real) * mask\n",
        "    return torch.sum(loss_)/torch.sum(mask)\n",
        "\n",
        "# Assuming the Transformer class is defined in a previous cell\n",
        "# Create an instance of the Transformer model\n",
        "# These are placeholder values; replace with actual model parameters\n",
        "num_layers = 6\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 2048\n",
        "input_vocab_size = 10000 # Placeholder\n",
        "target_vocab_size = 10000 # Placeholder\n",
        "pe_input = 1000 # Placeholder\n",
        "pe_target = 1000 # Placeholder\n",
        "rate = 0.1\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size,\n",
        "                          pe_input, pe_target, rate)\n",
        "\n",
        "\n",
        "# 2. Define the optimizer\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# 3. Implement a custom learning rate scheduler (basic example)\n",
        "class CustomSchedule(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = torch.tensor(d_model, dtype=torch.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        self.step_num += 1\n",
        "        arg1 = torch.tensor(self.step_num, dtype=torch.float32).pow(-0.5)\n",
        "        arg2 = torch.tensor(self.step_num * self.warmup_steps**(-1.5), dtype=torch.float32)\n",
        "        return [self.d_model.pow(-0.5) * torch.min(arg1, arg2).item()]\n",
        "\n",
        "learning_rate = CustomSchedule(optimizer, d_model=d_model) # Use the defined d_model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea9bcb6",
        "outputId": "6ef39e9c-0140-40bb-d7b2-ed2a9730c37f"
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class DummyDataLoader:\n",
        "    def __init__(self, num_batches=100, batch_size=64, input_seq_len=50, target_seq_len=50, input_vocab_size=10000, target_vocab_size=10000):\n",
        "        self.num_batches = num_batches\n",
        "        self.batch_size = batch_size\n",
        "        self.input_seq_len = input_seq_len\n",
        "        self.target_seq_len = target_seq_len\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.num_batches):\n",
        "            inp = torch.randint(1, self.input_vocab_size, (self.batch_size, self.input_seq_len))\n",
        "            tar = torch.randint(1, self.target_vocab_size, (self.batch_size, self.target_seq_len))\n",
        "            # Simulate padding with 0\n",
        "            inp[:, :5] = 0\n",
        "            tar[:, :5] = 0\n",
        "            yield inp, tar\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "train_dataloader = DummyDataLoader()\n",
        "val_dataloader = DummyDataLoader(num_batches=10)\n",
        "\n",
        "\n",
        "# Helper function to create masks\n",
        "def create_masks(inp, tar, device):\n",
        "    # Encoder padding mask (for the input)\n",
        "    enc_padding_mask = (inp == 0).unsqueeze(1).unsqueeze(1).to(device) # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "    # Decoder padding mask (for the second attention block in decoder)\n",
        "    dec_padding_mask = (inp == 0).unsqueeze(1).unsqueeze(1).to(device) # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "    # Look-ahead mask (for the first attention block in decoder)\n",
        "    size = tar.shape[1]\n",
        "    look_ahead_mask = torch.triu(torch.ones((size, size)), diagonal=1).bool().to(device)\n",
        "    look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(0) # (1, 1, seq_len, seq_len)\n",
        "\n",
        "    dec_target_padding_mask = (tar == 0).unsqueeze(1).unsqueeze(1).to(device) # (batch_size, 1, 1, seq_len)\n",
        "    look_ahead_mask = look_ahead_mask | dec_target_padding_mask\n",
        "\n",
        "    return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transformer.to(device)\n",
        "\n",
        "\n",
        "# 4. Define the training step function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_inp, device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions, _ = transformer(inp, tar_inp, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
        "    loss = loss_function(tar_real.to(device), predictions)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    learning_rate.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# 5. Define the evaluation step function\n",
        "def eval_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_inp, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions, _ = transformer(inp, tar_inp, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
        "        loss = loss_function(tar_real.to(device), predictions)\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# 6. Implement the main training loop\n",
        "EPOCHS = 10 # Placeholder\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_train_loss = 0\n",
        "    transformer.train()\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataloader):\n",
        "        inp, tar = inp.to(device), tar.to(device)\n",
        "        batch_loss = train_step(inp, tar)\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1} Train Loss {total_train_loss / len(train_dataloader):.4f}')\n",
        "\n",
        "    # Evaluation phase\n",
        "    total_eval_loss = 0\n",
        "    transformer.eval()\n",
        "    for (batch, (inp, tar)) in enumerate(val_dataloader):\n",
        "        inp, tar = inp.to(device), tar.to(device)\n",
        "        batch_loss = eval_step(inp, tar)\n",
        "        total_eval_loss += batch_loss\n",
        "\n",
        "    print(f'Epoch {epoch+1} Eval Loss {total_eval_loss / len(val_dataloader):.4f}')\n",
        "\n",
        "    # 7. Include functionality to save model checkpoints\n",
        "    # Placeholder for saving; replace with desired path and logic\n",
        "    if (epoch + 1) % 5 == 0: # Save every 5 epochs\n",
        "        torch.save(transformer.state_dict(), f'transformer_epoch_{epoch+1}.pt')\n",
        "        print(f'Model checkpoint saved for epoch {epoch+1}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 9.3952\n",
            "Epoch 1 Batch 50 Loss 9.3708\n",
            "Epoch 1 Train Loss 9.3624\n",
            "Epoch 1 Eval Loss 9.3234\n",
            "Epoch 2 Batch 0 Loss 9.3272\n",
            "Epoch 2 Batch 50 Loss 9.2821\n",
            "Epoch 2 Train Loss 9.2869\n",
            "Epoch 2 Eval Loss 9.2560\n",
            "Epoch 3 Batch 0 Loss 9.2604\n",
            "Epoch 3 Batch 50 Loss 9.2415\n",
            "Epoch 3 Train Loss 9.2425\n",
            "Epoch 3 Eval Loss 9.2301\n",
            "Epoch 4 Batch 0 Loss 9.2315\n",
            "Epoch 4 Batch 50 Loss 9.2363\n",
            "Epoch 4 Train Loss 9.2320\n",
            "Epoch 4 Eval Loss 9.2280\n",
            "Epoch 5 Batch 0 Loss 9.2343\n",
            "Epoch 5 Batch 50 Loss 9.2361\n",
            "Epoch 5 Train Loss 9.2332\n",
            "Epoch 5 Eval Loss 9.2308\n",
            "Model checkpoint saved for epoch 5\n",
            "Epoch 6 Batch 0 Loss 9.2353\n",
            "Epoch 6 Batch 50 Loss 9.2409\n",
            "Epoch 6 Train Loss 9.2360\n",
            "Epoch 6 Eval Loss 9.2323\n",
            "Epoch 7 Batch 0 Loss 9.2427\n",
            "Epoch 7 Batch 50 Loss 9.2346\n",
            "Epoch 7 Train Loss 9.2380\n",
            "Epoch 7 Eval Loss 9.2334\n",
            "Epoch 8 Batch 0 Loss 9.2339\n",
            "Epoch 8 Batch 50 Loss 9.2390\n",
            "Epoch 8 Train Loss 9.2385\n",
            "Epoch 8 Eval Loss 9.2341\n",
            "Epoch 9 Batch 0 Loss 9.2399\n",
            "Epoch 9 Batch 50 Loss 9.2348\n",
            "Epoch 9 Train Loss 9.2386\n",
            "Epoch 9 Eval Loss 9.2339\n",
            "Epoch 10 Batch 0 Loss 9.2500\n",
            "Epoch 10 Batch 50 Loss 9.2331\n",
            "Epoch 10 Train Loss 9.2387\n",
            "Epoch 10 Eval Loss 9.2336\n",
            "Model checkpoint saved for epoch 10\n"
          ]
        }
      ]
    }
  ]
}